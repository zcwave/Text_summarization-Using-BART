{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer)\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/zeche/.cache/huggingface/datasets/json/default-03e8c6085ad46111/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d7b4c9d66a4b3b9415058357ac6a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\zeche\\.cache\\huggingface\\datasets\\json\\default-03e8c6085ad46111\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-5c5539a03be0f26f.arrow\n",
      "Loading cached split indices for dataset at C:\\Users\\zeche\\.cache\\huggingface\\datasets\\json\\default-03e8c6085ad46111\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-df03e9ddd6008e3e.arrow and C:\\Users\\zeche\\.cache\\huggingface\\datasets\\json\\default-03e8c6085ad46111\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-8dfed0dc6bc8c7c1.arrow\n",
      "Loading cached split indices for dataset at C:\\Users\\zeche\\.cache\\huggingface\\datasets\\json\\default-03e8c6085ad46111\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-2f47dd92dfe95ca1.arrow and C:\\Users\\zeche\\.cache\\huggingface\\datasets\\json\\default-03e8c6085ad46111\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-021cbf4677f6cd53.arrow\n"
     ]
    }
   ],
   "source": [
    "data_sets = data_utils.load_data()\n",
    "# 加载tokenizer,中文bart使用bert的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fnlp/bart-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "\t这是一句中文测试。\n",
      "Tokenized:\n",
      "\tinput_ids: [101, 21498, 11009, 4896, 6392, 4941, 10843, 12806, 20486, 3566, 102]\n",
      "\ttoken_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\tattention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decode: \n",
      "\t[CLS] 这 是 一 句 中 文 测 试 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "encoded_input = '这是一句中文测试。'\n",
    "print(f'input: \\n\\t{encoded_input}')\n",
    "print('Tokenized:')\n",
    "encoded_input = tokenizer(encoded_input)\n",
    "for i in encoded_input.keys():\n",
    "    print(f'\\t{i}: {encoded_input[i]}')\n",
    "print(f'Decode: \\n\\t{tokenizer.decode(encoded_input[\"input_ids\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "\t 句子测试1。 句子测试2？ 句子测试3\n",
      "Tokenized:\n",
      "\tinput_ids: [101, 6392, 8263, 12806, 20486, 121, 3566, 102, 6392, 8263, 12806, 20486, 122, 25837, 102]\n",
      "\ttoken_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "\tattention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\tlabels: [101, 6392, 8263, 12806, 20486, 123, 102]\n",
      "Decode: \n",
      "\t[CLS] 句 子 测 试 1 。 [SEP] 句 子 测 试 2 ？ [SEP]\n"
     ]
    }
   ],
   "source": [
    "encoded_input = ['句子测试1。','句子测试2？', '句子测试3']\n",
    "print(f'input: \\n\\t', encoded_input[0], encoded_input[1], encoded_input[2])\n",
    "print('Tokenized:')\n",
    "encoded_input = tokenizer('句子测试1。','句子测试2？', '句子测试3')\n",
    "for i in encoded_input.keys():\n",
    "    print(f'\\t{i}: {encoded_input[i]}')\n",
    "print(f'Decode: \\n\\t{tokenizer.decode(encoded_input[\"input_ids\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "\t ['句子1', '句子长的2', '句子最长长长的3']\n",
      "Tokenized:\n",
      "\tinput_ids: \n",
      "\t\t\t[101, 6392, 8263, 121, 102, 0, 0, 0, 0, 0]\n",
      "\t\t\t[101, 6392, 8263, 22915, 15134, 122, 102, 0, 0, 0]\n",
      "\t\t\t[101, 6392, 8263, 11217, 22915, 22915, 22915, 15134, 123, 102]\n",
      "\tattention_mask: \n",
      "\t\t\t[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "\t\t\t[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "\t\t\t[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decode: \n",
      "\t\n",
      "\t[CLS] 句 子 1 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\t[CLS] 句 子 长 的 2 [SEP] [PAD] [PAD] [PAD]\n",
      "\t[CLS] 句 子 最 长 长 长 的 3 [SEP]\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"句子1\", \"句子长的2\", \"句子最长长长的3\"]\n",
    "print(f'input: \\n\\t', batch_sentences)\n",
    "batch = tokenizer(batch_sentences, padding=True)\n",
    "print('Tokenized:')\n",
    "for i in batch.keys():\n",
    "    if i == 'token_type_ids':\n",
    "        continue\n",
    "    print(f'\\t{i}: ')\n",
    "    for j in batch[i]:\n",
    "        print(f'\\t\\t\\t{j}')\n",
    "print(f'Decode: \\n\\t')\n",
    "for i in batch[\"input_ids\"]:\n",
    "    print(f'\\t{tokenizer.decode(i)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
